# Overview

The aim of this sample job is to create a set of simple JSON extract files from various csv sources based on specific conditionis (weekly aggregation)

# Intstellin

After cloning this repository, set up Python environment and install any dependencies as follows:

`export PYTHONPATH=$(pwd):$PYTHONPATH`
`cd ./<project_dir>`
`pip3 install -r requirements.txt`

# Running Job

On a machine with Spark cluster installed, run the following command:

`spark-submit main.py calendar.csv product.csv sales.csv store.csv --cluster_size 2`

The output should be 70 specific JSON files as described below and shown in json_files.zip.

# Data

* Source files sales.csv, calendar.csv, store.csv and product.csv
* Destination files format consumption.json


# Description 

* Sales data contains transactional data with the following columns netSales, salesUnits, dateId, productId and storeId. These fields need to be joined with other fields from other csvâ€™s to get the weekly aggregated value for each combination.

* Calendar data contains dateId, week number and year. (Criteria: sales.dateId should be equal to calendar.dateId to get weekNumber and year) 

* Product data contains productId vs hierarchy. (division, gender, category) (Criteria : sales.productId = product.productId)

* Store data contains store vs channel information. (Criteria:sales.storeId = store.storeId)

# Output
Output should be bunch of json files which is having the weekly aggregated values of net_sales and sales_units. The json files will have unique key. (Refer: consumption.json) 
